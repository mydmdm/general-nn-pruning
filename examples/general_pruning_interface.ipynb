{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pruner Interface\n",
    "\n",
    "## Motivation\n",
    "\n",
    "- Give users a clear picture on the model pruning algorithms and architecture\n",
    "- Facilitate on-demand customization of pruners (One Interface, Many Utils)\n",
    "- Explore the feasibility of hardware aware model compression (e.g., inference latency as the objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Generally, a model pruning algorithm is composed by the following key aspects:\n",
    "\n",
    "- **Target** to answer the question, \"*what (scope and granularity) to be pruned*\". The pruning granularity could be part of a tensor (e.g., blocks), or a computing concept comprised by multiple tensors (e.g., channels or heads). The finer grained pruning usually performs better (with less accuracy drop) but rarely gain large speedup ratio.\n",
    "- **Metric** to answer the question, \"*which of targets selected to be pruned*\". Frequently used metrics include *weights magnitudes* (e.g., $L_1$ or $L_2$ norm), *gradients* and *activations magnitudes*. There are also learnable metrics during fine tuning (*movement*).\n",
    "- **Allocation** to answer the question, \"*how to allocate the sparsity ratio among multiple targets*\". The basic allocation method is uniformly distributed, where all targets share the same sparsity ratio. Non-uniform sparsity allocators often apply searching and optimization (e.g., simulated annealing) to find a good solution.\n",
    "- **Scheduling** to answer the question, *whether the pruning is completed by one time or iteratively*. Frequently used schedulers include one-pass, linear, and AGP.\n",
    "\n",
    "The above aspects are orthogonal and users could combine them arbitrarily to realize existing algorithms (built-in in NNI) and explore new pruning algorithms. For hardware-aware neural network pruning, for example, pruning model to satisfy the inference latency on mobile device, requires another key aspect. \n",
    "\n",
    "- **Objective** to answer the question, \"*how to translate the objective (e.g., latency) to sparsity ratio*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The control flow is a nested loop, the outer of which is scheduler loop, and the inner loop is allocator loop.\n",
    "\n",
    "```python\n",
    "spar = ... # initial sparsity attributes\n",
    "while not scheduler.terminate(): # scheduling loop\n",
    "    step_objective = scheduler.get_objective() # generate objective for current step\n",
    "    while not is_achieved(step_objective, model, spar):\n",
    "        new_spar_specs = allocator.try_more()\n",
    "        pruner.prune(new_spar_specs)\n",
    "    speed_up(model, spar) # speeding up if necessary\n",
    "    fine_tune(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Interface, Many Utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target selector\n",
    "\n",
    "- *Tensor target*. This category mainly focuses on the important part inside a tensor (weights or activations). The typical granularity in this level is *block*, which is structured part of a tensor. The block is described by the *block sizes* and *axes*. The block size could be special values, such as single-element block (*fine grained*), *row* (or *column*) block, and even covering one or more dimensions. \n",
    "- *Operator target*. Some pruning granularity could be conceptual and larger than a single tensor. For example, the *head* in Multi-Head Attention (MHA) module, which is composed 4 weights tensors ($W_{Q,K,V,O}$). So does the *convolution channel*, which consists of weights and bias. Pruners dealing with such granularity need to understand the operator computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9bb069c333379a7579a782273aa0ac569fa62b5d4ff4394a12839f46a595457"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('py39': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
